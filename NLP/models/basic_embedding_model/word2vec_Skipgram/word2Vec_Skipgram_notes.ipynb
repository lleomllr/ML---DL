{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  **Skip-Gram Word2Vec**"
      ],
      "metadata": {
        "id": "Xrn2fb_lD1Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concept fondamental en NLP qui vise à traduire les mots en embedding vectors (vecteurs d'intégration). Ces vecteurs sont uniques dans la mesure où ils capturent les relations sémantiques que l'on troive dans le language naturel.\n",
        "\n",
        "Il existe une grande variété d'algo Word2Vec. Ici, on se concentre sur le Skip-Gram. Cet algo permet de représenter les mots sous forme de vecteurs en fonction de leur proximité spatiale par rapport à d'autres mots.\n",
        "\n",
        "Hypothèse : si les mots sont positionnés les uns à côté des autres, cela signifie qu'ils sont sémantiquement similaires.\n",
        "\n",
        "Cet algo fonctionne en itérant sur tous les mots du texte d'entrée et en optimisant la distance de leurs représentations vectorielles sur la base de leur contexte. Le contexte est défini comme une fenêtre de mots entourant un mot spécifique. Par exemple, dans la phrase suivante :\n",
        "\n",
        "\"I don't do magic, Morty, I do science\"\n",
        "\n",
        "Si le mot 'Morty' est le mot cible, et que la taille de notre fenêtre de mots est fixée à 1, les mots voisins seront 'magic' et 'I'. Toutes ces paires (mot cible, mot du contexte) peuvent ensuite être introduites dans le modèle, ce qui permet d'optimiser l'intégration des mots (word embeddings). Chaque paire de mots ajoute la même quantité au modèle.\n",
        "\n"
      ],
      "metadata": {
        "id": "SkL6ex5pEEHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comment Skip-Gram fonctionne ?"
      ],
      "metadata": {
        "id": "9MjWOrNVGbaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Structure du modèle"
      ],
      "metadata": {
        "id": "h_sJ_Fr8Ggwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle Skip-Gram est essentiellement un réseau de neurones peu profond avec une couche d'entrée (input layer), une couche d'intégration (embedding layer) et une couche de sortie (output layer).\n",
        "\n",
        "L'objectif du modèle est de produire un vecteur de distribution de proba en sortie à partir d'un mot cible. Ce vecteur (dont la somme est égale à 1) reflète la probabilité pour chaque mot d'apparaître dans la fenêtre contextuelle du mot cible. On veut alors que la proba soit élevée pour les mots qui partagent le même contexte et faible pour ceux qui ne le partagent pas.\n",
        "\n",
        "Une fois entraîné, nous n'avons besoin que des poids du modèle.\n",
        "\n",
        "**Comment prend-il les paires (mot cible, mot du contexte) ?**\n",
        "\n",
        "Les mots cibles fonctionnent comme l'entrée (X_train), tandis que les mots du contexte fonctionnent comme la sortie (y_train)."
      ],
      "metadata": {
        "id": "169uChUUGexb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Loss Function"
      ],
      "metadata": {
        "id": "GPTitFRZHyn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pour obtenir des vectors embeddings utiles, il faut optimiser les poids du modèle, initialement fixés de manière totalement aléatoire. Le processus d'optimisation est réalisé afin de minimiser la fonction de perte :\n",
        "\n",
        "$J = - Σ_{t=1}^{T} Σ_{-m \\leq j \\leq m} log(P(w_{t+j}|w_t))$\n",
        "\n",
        "où T = longueur du texte, m = taille de la fenêtre de mots, P(wt+j|wt) = la proba d'obtenir un mot de contexte à partir d'un mot cible.\n",
        "\n",
        "**Comment lire cette équation ?**\n",
        "\n",
        "Il s'agit d'une boucle imbriquée, qui itère sur toutes les paires (mot cible, mot de contexte) et à additionne les proba. Le signe '-' est souvent utilisé en ML => minimiser la valeur de la fonction de perte."
      ],
      "metadata": {
        "id": "XI6nBj1yH9SF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Comment ces probas sont-elles calculées ?"
      ],
      "metadata": {
        "id": "AmQNEZRhKAAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour calculer la distribution de proba, utilisation de la fonction Softmax qui prend en compte le produit point du vecteur d'intégration de la cible et des vecteurs d'intégration de chaque mot du vocab :\n",
        "\n",
        "$u^{T}v = u \\cdot v = Σ_{i=1}^{n} u_i v_i$\n",
        "\n",
        "$p$(context word/target word) $= \\frac{exp(u_{target}^T v_{context})}{Σ_{w=1}^{Words} (exp(u_{target}^T v_w))}$"
      ],
      "metadata": {
        "id": "Hu43zzTRKGO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compléments"
      ],
      "metadata": {
        "id": "nWjVd4K6Of9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Objectif du modèle : Contrairement à un réseau de neurones traditionnel, utilisé pour la classification, l'objectif est d'entraîner un réseau afin d'en extraire les poids. En résumé, il n'est pas nécessaire de l'utiliser de la manière traditionnelle : input -> model -> output.\n",
        "\n",
        "\n",
        "2. Concernant les poids du modèle : Contrairement à un réseau de neurones classique, où les poids sont utilisés pour traiter n'importe quelle entrée, le modèle attribue des poids uniques à chaque mot. => les poids du modèle ne peuvent pas être généralisés pour traiter de nouveaux mots ou de nouvelles entrées de la même manière. A la place, les poids servent d'encastrements spécifiques pour chaque mot.\n",
        "\n",
        "\n",
        "3. L'utilisation d'un encodage à chaud n'est pas nécessaire : Chaque mot est converti en un vecteur d'encodage à chaud (one-hot encoding vector) transmis au modèle (comme 'rick' = [0,0,0,..,1,...0]). Bien qu'il s'agisse de la norme, l'utilisation d'un vecteur peu dense n'a guère de sens. A la place, transmission de l'index du mot -> moins gourmand en mémoire."
      ],
      "metadata": {
        "id": "zva8KJpIOzdr"
      }
    }
  ]
}